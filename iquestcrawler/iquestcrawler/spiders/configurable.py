import os
import uuid
import logging

from ..utils.upstash_vector_store import UpstashVectorStore
from ..utils.config import text_splitter_config, crawler_config

# Use basic Spider
# from scrapy.spiders import CrawlSpider, Rule
from scrapy.spiders import Spider 
from scrapy import Request
# LinkExtractor is not needed for basic Spider approach
# from scrapy.linkextractors import LinkExtractor

from langchain.text_splitter import RecursiveCharacterTextSplitter


# Change base class to Spider
class ConfigurableSpider(Spider):

    name = "configurable"
    start_urls = crawler_config["start_urls"]
    # Define allowed domains based on start_urls - adjust if needed
    allowed_domains = [url.split("//")[1].split("/")[0] for url in crawler_config["start_urls"]]
    # Rules are not used in basic Spider
    # rules = (
    #     Rule(
    #         LinkExtractor(
    #             **crawler_config["link_extractor"]
    #         ),
    #         callback="parse_page",
    #         follow=True, # to enable following links on each page when callback is provided
    #         process_request='use_playwright' # Tell Rule to use Playwright
    #     ),
    # )

    def __init__(self, *a, **kw):
        super().__init__(*a, **kw)

        self.vectorstore = UpstashVectorStore(
            url=os.environ.get("UPSTASH_VECTOR_REST_URL"),
            token=os.environ.get("UPSTASH_VECTOR_REST_TOKEN")
        )

        print(
            f"Creating a vector index at {os.environ.get('UPSTASH_VECTOR_REST_URL')}.\n"
            f" Vector store info before crawl: {self.vectorstore.index.info()}"
        )

        self.text_splitter = RecursiveCharacterTextSplitter(
            **text_splitter_config
        )

        self._disable_loggers()

    def _disable_loggers(self):
        """
        disables some of the loggers to keep the log clean
        """

        disable_loggers = [
            "scrapy.spidermiddlewares.depth",
            # "protego", # Keep protego logs if needed for robots.txt debugging
            "httpcore.http11",
            "httpx",
            "openai._base_client",
            "urllib3.connectionpool",
            "scrapy.spidermiddlewares.offsite", # Disable OffsiteMiddleware logs unless debugging domain issues
        ]
        for logger in disable_loggers:
            logging.getLogger(logger).setLevel(logging.WARNING)

    def start_requests(self):
        """
        Use Playwright for the initial start URLs.
        """
        for url in self.start_urls:
            # Ensure Playwright is used for the first request
            yield Request(url, callback=self.parse, meta={'playwright': True})

    # This method is no longer needed with basic Spider
    # def use_playwright(self, request, response):
    #     """
    #     Middleware function to add Playwright meta flag to requests
    #     generated by the LinkExtractor rule.
    #     """
    #     request.meta['playwright'] = True
    #     return request

    # Rename parse_page to parse (default callback for basic Spider)
    def parse(self, response):
        """
        Processes the response, extracts text, adds to vector store,
        and yields new requests for found links.
        """
        self.logger.info(f"Processing page: {response.url} Status: {response.status}")

        # --- Text Extraction and Vector Store Logic (same as before) ---
        text_content = response.xpath('//p//text()').getall() # Get text from all p tags
        text_content = ' \n'.join(filter(None, (t.strip() for t in text_content)))

        if text_content:
            documents = self.text_splitter.split_text(text_content)
            if documents:
                self.logger.info(f"Adding {len(documents)} document chunks from {response.url}")
                self.vectorstore.add(
                    ids=[str(uuid.uuid4())[:8] for _ in documents],
                    documents=documents,
                    link=response.url
                )
            else:
                 self.logger.warning(f"No document chunks generated for {response.url}")
        else:
             self.logger.warning(f"No text content extracted from {response.url}")
        # --- End Text Extraction ---

        # --- Link Following Logic ---
        # Extract all links on the page
        links = response.xpath('//a/@href').getall()
        for link in links:
            # Construct absolute URL
            absolute_link = response.urljoin(link)
            # Basic filtering (can be enhanced)
            if any(domain in absolute_link for domain in self.allowed_domains) and "#" not in absolute_link and "?" not in absolute_link.split("#")[0]:
                 self.logger.debug(f"Following link: {absolute_link}")
                 # Yield a new Request, ensuring Playwright is used
                 yield Request(absolute_link, callback=self.parse, meta={'playwright': True})
            # else:
            #      self.logger.debug(f"Skipping link: {absolute_link}")
        # --- End Link Following ---
